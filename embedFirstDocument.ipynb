{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use OpenAI Embeddings to Find Closest Word \n",
    "###  Given two words we can \"guess\" the third word \n",
    "####            [{1:\"king\",2:\"woman\",3:\"queen\"},\n",
    "####             {1:\"Paris\",2:\"England\",3:\"London\"},\n",
    "####             {1:\"Spain\",2:\"Tokyo\",3:\"Japan\"},\n",
    "####             {1:\"Apple\",2:\"phone\",3:\"iPhone\"},\n",
    "####             {1:\"fish\",2:\"home\",3:\"aquarium\"},\n",
    "####             {1:\"read\",2:\"pleasure\",3:\"book\"},\n",
    "####            {1:\"computer\",2:\"network\",3:\"internet\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_triples = [{1:\"king\",2:\"woman\",3:\"queen\"},\n",
    "              {1:\"Paris\",2:\"England\",3:\"London\"},\n",
    "              {1:\"Spain\",2:\"Tokyo\",3:\"Japan\"},\n",
    "              {1:\"Apple\",2:\"phone\",3:\"iPhone\"},\n",
    "              {1:\"fish\",2:\"chips\",3:\"meal\"},\n",
    "              {1:\"read\",2:\"pleasure\",3:\"book\"},\n",
    "              {1:\"walk\",2:\"exercise\",3:\"run\"},\n",
    "              {1:\"computer\",2:\"network\",3:\"internet\"}\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all words\n",
    "words = []\n",
    "for row in word_triples:\n",
    "    words.append(row[1])\n",
    "    words.append(row[2])\n",
    "    words.append(row[3])\n",
    "print(words)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(word):\n",
    "    response = openai.embeddings.create(\n",
    "    input=word,\n",
    "    model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for all words\n",
    "embeddings = {}\n",
    "for word in words:\n",
    "    embeddings[word] = np.array(getEmbedding(word))\n",
    "\n",
    "# normalize embeddings\n",
    "for word in words:\n",
    "    embeddings[word] = embeddings[word]/np.linalg.norm(embeddings[word])\n",
    "    \n",
    "import csv\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"embedFirstDocument.csv\"\n",
    "\n",
    "# Write the word and embedding to the CSV file\n",
    "with open(file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Word', 'Embedding'])  # Write the header\n",
    "    for word, embedding in embeddings.items():\n",
    "        writer.writerow([word, embedding.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_nearest_word_to_sum(word1,word2):\n",
    "    # Calculate cosine similarity between the input embedding and all embeddings \n",
    "   # get the sum of the two embeddings vectors np.array\n",
    "    esum = embeddings[word1] + embeddings[word2]\n",
    "    esum = esum/np.linalg.norm(esum)\n",
    "    # find the nearest word not in the input\n",
    "    similarities = {}\n",
    "    for word in words:\n",
    "        if word != word1 and word != word2:\n",
    "            similarities[word] = np.dot(esum, embeddings[word])\n",
    "    nearest_word = max(similarities, key=similarities.get)\n",
    "    return nearest_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the word triples and find the nearest word to the sum of the first and second word\n",
    "for row in word_triples:\n",
    "    word1 = row[1]\n",
    "    word2 = row[2]\n",
    "    word3 = row[3]\n",
    "    nearest_word = find_nearest_word_to_sum(word1,word2)\n",
    "    print(\"The nearest word to\", word1, \"+\", word2, \"is\", nearest_word, \"and the actual word is\", word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add first two embeddings and divide by 2 in each row\n",
    "# add each prection vecctor to matrix of predictions\n",
    "matrix = np.array([])\n",
    "i = 1\n",
    "for row in word_triples:\n",
    "    r = (embeddings[row[1]] + embeddings[row[2]]) / 2\n",
    "    # add this vector as a row to matrix_of_predictions\n",
    "    if i == 1:\n",
    "        matrix = r\n",
    "    else:\n",
    "        matrix = np.vstack((matrix, r))\n",
    "    i += 1\n",
    "for row in word_triples:\n",
    "    matrix = np.vstack((matrix, embeddings[row[3]]))\n",
    "print(matrix.shape)\n",
    "# reshape predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=3)\n",
    "\n",
    "X_2d = tsne.fit_transform(matrix)\n",
    "colors = [\"red\", \"darkorange\",  \"blue\", \"darkgreen\",\"purple\"]\n",
    "# get back original dataframe with columns and loan_status\n",
    "x = [x for x,y in X_2d]\n",
    "y = [y for x,y in X_2d]\n",
    "color_indices = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1]\n",
    "colormap = matplotlib.colors.ListedColormap(colors)\n",
    "plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using openai to tokenize the text\n",
    "import tiktoken\n",
    "text = \"The quick brown fox jumped over the lazy dog.\" \n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens)\n",
    "text_new = encoding.decode(tokens)\n",
    "print(text_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
